https://www.bilibili.com/video/BV1K3QAY9E1F/?spm_id_from=333.337.search-card.all.click&vd_source=3d50341f547faf8df242a214b04f2d86

how transform llms work
language processing 
instructors（instructors） for this course 
Hands-on Large Language Models
the underlying architecture of LLMs and provided insightful explanations of transformers 
easy to understand 
transformer based LLMs
present that information in person
read through papers describing models and understand the details 
intuitions 直觉
accessible to all 
transformer architecture  2017 
Attention is all you need.
The idea was to say input an english sentence and have the network output a German sentence.
turned out  结果是
herald 先驱
And so this helped herald the early rise of large language models.
two main parts.
encoder decoder
The encoder preprocesses the entire input English text to extract the context needed to perform the translation.
the basis for the bert model. 
The encoder model provides rich context-sensitive representations of the input

The decoder model performs text generation tasks,such as summarizing text.

delve into 深入研究
Attention 是一种机制，让模型在处理每个词时能“关注”输入中最相关的部分，从而更好理解上下文。
comprise words 
fed into the llm
you gain intuition 
generates a text in response by generating one token at a time.
scale well on GPU

bag of words
large sparse vectors 