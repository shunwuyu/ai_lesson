{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "894.08s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: arxiv in /opt/homebrew/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/homebrew/lib/python3.10/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/homebrew/lib/python3.10/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /opt/homebrew/lib/python3.10/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting openai\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/81/d2/e3992bb7c6641b765c1008e3c96e076e0b50381be2cce344e6ff177bad80/openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/01/07/7bf6022c5a152fca767cf5c086bb41f7c28f70cf33ad259d023b53c0b858/jiter-0.9.0-cp310-cp310-macosx_11_0_arm64.whl (321 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/shunwuyu/Library/Python/3.10/lib/python/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/shunwuyu/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a3/44/3f0b95fafdaca04a483c4e685fe437c6891001bf3ce8b2fded82b9ea3aa1/pydantic_core-2.33.2-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.9.0 openai-1.79.0 pydantic-2.11.4 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "  \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "  \"\"\"\n",
    "\n",
    "  client = arxiv.Client()\n",
    "  search = arxiv.Search(\n",
    "      query = topic,\n",
    "      max_results = max_results,\n",
    "      sort_by = arxiv.SortCriterion.Relevance\n",
    "  )\n",
    "  papers = client.results(search)\n",
    "\n",
    "  path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "  os.makedirs(path, exist_ok=True)\n",
    "  file_path = os.path.join(path, \"papers_info.json\")\n",
    "  # Try to load existing papers info\n",
    "  try:\n",
    "      with open(file_path, \"r\") as json_file:\n",
    "          papers_info = json.load(json_file)\n",
    "  except (FileNotFoundError, json.JSONDecodeError):\n",
    "      papers_info = {}\n",
    "\n",
    "  paper_ids = []\n",
    "  for paper in papers:\n",
    "    paper_ids.append(paper.get_short_id())\n",
    "    paper_info = {\n",
    "        'title': paper.title,\n",
    "        'authors': [author.name for author in paper.authors],\n",
    "        'summary': paper.summary,\n",
    "        'pdf_url': paper.pdf_url,\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    papers_info[paper.get_short_id()] = paper_info\n",
    "\n",
    "  with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "\n",
    "  print(f\"Results are saved in: {file_path}\")\n",
    "\n",
    "  return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/computers/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1310.7911v2',\n",
       " 'math/9711204v1',\n",
       " '2208.00733v1',\n",
       " '2504.07020v1',\n",
       " '2403.03925v1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "\n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "\n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    "\n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Compact manifolds with computable boundaries\",\\n  \"authors\": [\\n    \"Zvonko Iljazovic\"\\n  ],\\n  \"summary\": \"We investigate conditions under which a co-computably enumerable closed set\\\\nin a computable metric space is computable and prove that in each locally\\\\ncomputable computable metric space each co-computably enumerable compact\\\\nmanifold with computable boundary is computable. In fact, we examine the notion\\\\nof a semi-computable compact set and we prove a more general result: in any\\\\ncomputable metric space each semi-computable compact manifold with computable\\\\nboundary is computable. In particular, each semi-computable compact\\\\n(boundaryless) manifold is computable.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1310.7911v2\",\\n  \"published\": \"2013-10-29\"\\n}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('1310.7911v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {   \n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_papers\",\n",
    "            \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "            \"parameters\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {   \n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_info\",\n",
    "            \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "            \"parameters\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}\n",
    "\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "  result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "  if result in None:\n",
    "    result = \"The operation completed but didn't return any results.\"\n",
    "  elif isinstance(result, list):\n",
    "    result = \", \".join(result)\n",
    "  elif isinstance(result, dict):\n",
    "    result = json.dumps(result, indent = 2)\n",
    "  else:\n",
    "    result = str(result)\n",
    "\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"sk-lnvdfaibxybnznhrmbmugwvlnrjmvquaketukuymhzvdyvzo\", base_url=\"https://api.siliconflow.cn/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='0196e2a4a570bc6a91645a5a3ad562b0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' 编写一个Python异步爬虫可以大大提高爬取数据的效率，尤其是在需要处理大量请求的情况下。Python的`asyncio`库和`aiohttp`库是实现异步爬虫的关键工具。下面是一个详细的教程，包括代码示例和注意事项。\\n\\n### 环境准备\\n\\n首先，确保你已经安装了`asyncio`和`aiohttp`库。你可以使用`pip`来安装：\\n\\n```bash\\npip install aiohttp\\n```\\n\\n### 基本概念\\n\\n- **异步编程**：异步编程允许程序在等待某些操作（如网络请求）完成时执行其他任务，从而提高效率。\\n- **事件循环**：`asyncio`库的核心是事件循环，它负责调度和执行异步任务。\\n- **协程**：协程是一种使用`async def`定义的函数，可以暂停和恢复执行。\\n\\n### 代码示例\\n\\n下面是一个简单的异步爬虫示例，它从多个URL获取网页内容并打印出来。\\n\\n```python\\nimport aiohttp\\nimport asyncio\\n\\nasync def fetch(session, url):\\n    async with session.get(url) as response:\\n        return await response.text()\\n\\nasync def main(urls):\\n    async with aiohttp.ClientSession() as session:\\n        tasks = []\\n        for url in urls:\\n            task = asyncio.create_task(fetch(session, url))\\n            tasks.append(task)\\n        results = await asyncio.gather(*tasks)\\n        for result in results:\\n            print(result[:100])  # 打印前100个字符\\n\\nif __name__ == \"__main__\":\\n    urls = [\\n        \"https://www.example.com\",\\n        \"https://www.python.org\",\\n        \"https://www.github.com\"\\n    ]\\n    asyncio.run(main(urls))\\n```\\n\\n### 代码解释\\n\\n1. **fetch函数**：这个函数使用`aiohttp`库的`session.get`方法发送HTTP GET请求，并返回响应的文本内容。\\n2. **main函数**：这个函数创建一个`aiohttp.ClientSession`对象，并为每个URL创建一个协程任务。然后使用`asyncio.gather`并发执行这些任务，并收集结果。\\n3. **事件循环**：`asyncio.run(main(urls))`启动事件循环，执行`main`函数。\\n\\n### 注意事项\\n\\n1. **并发控制**：在处理大量请求时，应该注意并发控制，避免对目标网站造成过大压力。可以使用`Semaphore`来限制并发请求数量。\\n2. **错误处理**：在网络请求中，可能会遇到各种错误，如连接超时、HTTP错误等。应该添加适当的错误处理逻辑。\\n3. **用户代理**：为了模拟正常的用户行为，应该设置用户代理（User-Agent）。\\n4. **遵守法律和网站规则**：在爬取数据时，应该遵守相关法律法规和网站的`robots.txt`文件中的规定。\\n\\n### 进阶示例\\n\\n下面是一个更复杂的示例，包含并发控制和错误处理。\\n\\n```python\\nimport aiohttp\\nimport asyncio\\nfrom aiohttp import ClientError\\n\\nasync def fetch(session, url, semaphore):\\n    async with semaphore:\\n        try:\\n            async with session.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}) as response:\\n                response.raise_for_status()\\n                return await response.text()\\n        except ClientError as e:\\n            print(f\"Error fetching {url}: {e}\")\\n            return None\\n\\nasync def main(urls, max_concurrent_requests=5):\\n    semaphore = asyncio.Semaphore(max_concurrent_requests)\\n    async with aiohttp.ClientSession() as session:\\n        tasks = []\\n        for url in urls:\\n            task = asyncio.create_task(fetch(session, url, semaphore))\\n            tasks.append(task)\\n        results = await asyncio.gather(*tasks)\\n        for result in results:\\n            if result:\\n                print(result[:100])  # 打印前100个字符\\n\\nif __name__ == \"__main__\":\\n    urls = [\\n        \"https://www.example.com\",\\n        \"https://www.python.org\",\\n        \"https://www.github.com\"\\n    ]\\n    asyncio.run(main(urls))\\n```\\n\\n在这个示例中，我们使用`Semaphore`来限制并发请求数量，并添加了错误处理逻辑。\\n\\n通过这些示例和注意事项，你应该能够编写一个基本的异步爬虫。希望这个教程对你有所帮助！', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1747559163, model='Qwen/Qwen2.5-Coder-32B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='', usage=CompletionUsage(completion_tokens=919, prompt_tokens=43, total_tokens=962, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(  \n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",  \n",
    "    messages=[{  \n",
    "        \"role\": \"user\",  \n",
    "        \"content\": \"编写Python异步爬虫教程，包含代码示例和注意事项\"  \n",
    "    }],  \n",
    "    temperature=0.7,  \n",
    "    max_tokens=4096  \n",
    ")  \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    response = client.chat.completions.create(max_tokens = 2024,\n",
    "                                  model = 'deepseek-ai/DeepSeek-V2.5', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages)\n",
    "    print(response.content)\n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "\n",
    "        for content in response.choices:\n",
    "            if content.type == 'text':\n",
    "                \n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "                \n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "            \n",
    "            elif content.type == 'tool_use':\n",
    "                \n",
    "                assistant_content.append(content)\n",
    "                messages.append({'role': 'assistant', 'content': assistant_content})\n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append({\"role\": \"user\", \n",
    "                                  \"content\": [\n",
    "                                      {\n",
    "                                          \"type\": \"tool_result\",\n",
    "                                          \"tool_use_id\": tool_id,\n",
    "                                          \"content\": result\n",
    "                                      }\n",
    "                                  ]\n",
    "                                })\n",
    "                response = client.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages) \n",
    "                \n",
    "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
    "                    print(response.content[0].text)\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='0196e2a65b60fb09b201808beb7fd0b8', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0196e2a66103579041d77958c8296162', function=Function(arguments='{\"topic\":\"LLM interpretability\",\"max_results\":5}', name='search_papers'), type='function')]))], created=1747559275, model='deepseek-ai/DeepSeek-V2.5', object='chat.completion', service_tier=None, system_fingerprint='', usage=CompletionUsage(completion_tokens=29, prompt_tokens=161, total_tokens=190, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletion' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLLM interpretability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mprocess_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m process_query:\n\u001b[1;32m     12\u001b[0m     assistant_content \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m content\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28mprint\u001b[39m(content\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pydantic/main.py:989\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatCompletion' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "process_query(\"LLM interpretability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
