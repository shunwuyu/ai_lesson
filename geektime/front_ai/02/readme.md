# 流式传输

- 调用LLM 过程是怎么样的？
  - 用户在浏览器提交数据
  - 大模型通常是需要实时推理的， 需要时间
    问题复杂度， 生成的 token 数量
    撰写一篇千字的作文
  - 服务端完成推理
  - 结果以 JSON 数据格式通过标准的 HTTP 协议返回给前端

  前端的响应速度并没有太慢，这正是因为它们默认采用了流式（streaming）传输，不必等到整个推理完成再将内容返回，而是可以将逐个 token 实时返回给前端，这样就大大减少了响应时间。

## 流式
- 为我初始化一个vue + vite + ts 项目


